<!-- id: 66257055 -->
# External Resources for the Container Service

One of the key design principles of the container service is that images be self-contained and immutable. An image contains a script or executable along with everything that script or executable needs in order to run. The image is immutable; it runs the same every time. It is sharable; somewhere else can pull down the image, if it is public, or request the image be exported and transferred, and they can be sure their copy is the exact same image as the original. And it is auditable; each image can be uniquely identified and preserved for records and diagnostics.

But this design comes with costs. For instance, the size of the docker image can be prohibitively large. In our practice, this has come up multiple times in relation to atlases. Some scripts want to take their input images ("images" here meaning a file representation of a 3D or 4D scene, not "images" in the docker sense) and register them against an atlas. The atlas files are a requirement for the script to run, so by our design principle they should be included in the (docker) image. And the atlas images are big, but not so big as to be worrisome. But say a script could register against several different atlases depending on an input argument. Including all these atlases in the image can greatly increase its size.

That size cost could be avoided by storing the atlas files outside the docker image and mounting them into the container at run time. In our lab, we already have the atlas files available to all the machines on our network. Other labs probably have a similar setup. It seems wasteful to have the atlases available, but also keep a copy inside each docker image that needs to use them. Why can't the container service launch a container with a mount to the atlases we already have?

Well, the immediate reason we can't is that the code doesn't exist. The container service can only mount directories and files inside the archive, the paths to which it found by referencing XNAT objects. But that is a dodge; that code could easily be changed. The ultimate reason we can't do this comes back to the key design principle discussed above. Once the image is no longer self-contained, once some of its required resources are assumed to be mounted in at runtime, then we lose a lot of nice features. The image is no longer immutable, sharable, or auditable. But maybe that is ok?

Now let's discuss how this feature could be implemented.

In order to mount files from somewhere, we need to have a path. Obviously we can't accept a path from the user launching the container, because A. that is a giant security hole, and B. users probably don't know the paths. We can't put the path into the command, because the command is sharable while the path to a resource on my machine may be different from the path on your machine. So we need to have a "resource registry" in XNAT. We need an admin to define what the resource is, where is its path, maybe some related information about it. More on this related information later.

If we have an external resource's path registered in XNAT, the container service will be able to read that path and mount it when launching a container. But how does the container service know it needs to do that? The command will have to have a reference to an external resource to be mounted. That way, when a launch request comes in and container service reads the command, it will see that it needs to go find some external resource and mount it into the container. What form should this command reference take? We can't reference some kind of ID for the external resource; this is likely to be system-specific and thus not sharable. We could reference it by a name, but this seems dangerous; how do we know that the same resource will be created with the same name on different systems?

On the other hand, maybe an ID or a name *could* be used. The entities referenced, however, would not live on an individual XNAT but on some yet-to-be-built central resource repository. When a user registers a resource there, it would be given a unique identifier. That identifier could be referenced in a command to tell the container service that the corresponding resource files should be used. If the resource does not exist on the user's local XNAT, it could be downloaded from the central resource repository. Or, if the user already has the files (in the case of an atlas, this is likely), then the user could manually register the resource into their XNAT and reference the identifier from the central repository. Perhaps XNAT will need to verify a checksum of the local files against the central files before approving the connection.

These are my thoughts about this feature. It is doable, but to be done well in the way I envision it requires infrastructure that doesn't exist: a central public resource repository, and an XNAT plugin / native XNAT code to interact with it.
